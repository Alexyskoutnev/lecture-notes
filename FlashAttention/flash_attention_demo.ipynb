{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6ee840",
   "metadata": {},
   "source": [
    "# Flash Attention: Making Transformers Memory Efficient\n",
    "\n",
    "This notebook demonstrates the key differences between standard attention \n",
    "and Flash Attention, focusing on memory efficiency and computational advantages.\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Understand memory bottlenecks in standard attention\n",
    "2. Learn how Flash Attention solves these problems\n",
    "3. Compare implementations and performance\n",
    "4. Visualize attention patterns\n",
    "\n",
    "## Prerequisites:\n",
    "- Basic understanding of transformer attention\n",
    "- Familiarity with PyTorch tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd82638",
   "metadata": {},
   "source": [
    "# Vanilla Transformer: Attention Mechanism Explained\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "**Attention** is a mechanism that allows a model to focus on different parts of the input when processing each element. Think of it like **selective focus** - when you read a sentence, you might pay more attention to certain words based on what you're trying to understand.\n",
    "\n",
    "### Core Idea\n",
    "> *\"How much should each word pay attention to every other word?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## The Attention Formula\n",
    "\n",
    "The fundamental attention mechanism can be expressed as:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n",
    "```\n",
    "\n",
    "### Three Key Components:\n",
    "\n",
    "- **Query (Q)**: *\"What am I looking for?\"*\n",
    "- **Key (K)**: *\"What information is available?\"*  \n",
    "- **Value (V)**: *\"What is the actual content?\"*\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Process\n",
    "\n",
    "### 1. **Compute Similarity Scores**\n",
    "```\n",
    "Scores = Q @ K^T / √d_k\n",
    "```\n",
    "- Calculate how similar each query is to each key\n",
    "- Scale by √d_k to prevent softmax saturation\n",
    "- Results in an [L × L] attention matrix\n",
    "\n",
    "### 2. **Apply Softmax**\n",
    "```\n",
    "Attention_Weights = softmax(Scores)\n",
    "```\n",
    "- Convert scores to probabilities\n",
    "- Each row sums to 1.0\n",
    "- High scores → high attention weights\n",
    "\n",
    "### 3. **Weighted Sum of Values**\n",
    "```\n",
    "Output = Attention_Weights @ V\n",
    "```\n",
    "- Combine values based on attention weights\n",
    "- Each output is a weighted average of all values\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "Instead of one attention mechanism, use **multiple heads** in parallel:\n",
    "\n",
    "### Why Multiple Heads?\n",
    "- **Different perspectives**: Each head can focus on different types of relationships\n",
    "- **Richer representations**: Capture various linguistic patterns simultaneously\n",
    "- **Increased capacity**: More parameters to learn complex patterns\n",
    "\n",
    "### Process:\n",
    "1. **Split** embeddings into H heads: `[B, L, D] → [B, H, L, D/H]`\n",
    "2. **Compute** attention for each head independently\n",
    "3. **Concatenate** head outputs: `[B, H, L, D/H] → [B, L, D]`\n",
    "4. **Project** through final linear layer\n",
    "\n",
    "---\n",
    "\n",
    "## Self-Attention vs Cross-Attention\n",
    "\n",
    "### Self-Attention\n",
    "- **Q, K, V** all come from the **same sequence**\n",
    "- *\"How do words in this sentence relate to each other?\"*\n",
    "- Used in encoder and decoder blocks\n",
    "\n",
    "### Cross-Attention\n",
    "- **Q** from target sequence, **K, V** from source sequence\n",
    "- *\"How does each target word relate to source words?\"*\n",
    "- Used in encoder-decoder architectures (translation, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Causal (Masked) Attention\n",
    "\n",
    "For **autoregressive** tasks (language modeling), prevent looking at future tokens:\n",
    "\n",
    "### Causal Mask:\n",
    "```\n",
    "Mask = triu(ones(L, L), diagonal=1)\n",
    "Scores.masked_fill_(Mask, -∞)\n",
    "```\n",
    "\n",
    "### Result:\n",
    "- Position `i` can only attend to positions `≤ i`\n",
    "- Creates lower-triangular attention pattern\n",
    "- Essential for language generation\n",
    "\n",
    "---\n",
    "\n",
    "## Memory Complexity: The Problem\n",
    "\n",
    "### Standard Attention Memory Usage:\n",
    "- **Attention Matrix**: `O(L²)` - stores all pairwise similarities\n",
    "- **Major Bottleneck**: For L=4096, needs ~67MB just for attention weights\n",
    "- **Quadratic Scaling**: Memory grows as sequence length squared\n",
    "\n",
    "### Example Memory Usage:\n",
    "| Sequence Length | Attention Matrix Memory |\n",
    "|-----------------|-------------------------|\n",
    "| 512             | 1 MB                    |\n",
    "| 1024            | 4 MB                    |\n",
    "| 2048            | 16 MB                   |\n",
    "| 4096            | 64 MB                   |\n",
    "| 8192            | 256 MB                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Intuition\n",
    "\n",
    "### Example: \"The cat sat on the mat\"\n",
    "\n",
    "When processing \"sat\":\n",
    "- **High attention** to \"cat\" (subject-verb relationship)\n",
    "- **Medium attention** to \"mat\" (location context)\n",
    "- **Low attention** to \"the\" (less semantically important)\n",
    "\n",
    "### Visualization:\n",
    "```\n",
    "    The  cat  sat  on  the  mat\n",
    "The [0.1][0.1][0.2][0.2][0.2][0.2]\n",
    "cat [0.1][0.3][0.4][0.1][0.0][0.1]\n",
    "sat [0.0][0.6][0.2][0.1][0.0][0.1]  ← \"sat\" pays most attention to \"cat\"\n",
    "on  [0.1][0.1][0.1][0.2][0.1][0.4]\n",
    "the [0.1][0.1][0.1][0.1][0.3][0.3]\n",
    "mat [0.1][0.2][0.1][0.3][0.1][0.2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Transformer Architecture Context\n",
    "\n",
    "### Encoder Block:\n",
    "```\n",
    "Input → Self-Attention → Add&Norm → Feed-Forward → Add&Norm → Output\n",
    "```\n",
    "\n",
    "### Decoder Block:\n",
    "```\n",
    "Input → Causal Self-Attention → Add&Norm → Cross-Attention → Add&Norm → Feed-Forward → Add&Norm → Output\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "- **Residual connections**: `output = layer(input) + input`\n",
    "- **Layer normalization**: Stabilize training\n",
    "- **Feed-forward networks**: Position-wise processing\n",
    "- **Multiple layers**: Stack for increased capacity\n",
    "\n",
    "---\n",
    "\n",
    "## Common Variations\n",
    "\n",
    "### 1. **Scaled Dot-Product Attention** (Standard)\n",
    "```\n",
    "Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "```\n",
    "\n",
    "### 2. **Multi-Query Attention**\n",
    "- Share K,V across heads, unique Q per head\n",
    "- Reduces memory and computation\n",
    "\n",
    "### 4. **Flash Attention**\n",
    "- Block-wise computation\n",
    "- Reduces memory from O(L²) to O(L)\n",
    "\n",
    "---\n",
    "\n",
    "## Why Attention Works\n",
    "\n",
    "### 1. **Flexible Relationships**\n",
    "- Can model any word-to-word relationship\n",
    "- Not limited by sequential processing\n",
    "\n",
    "### 2. **Interpretable Weights**\n",
    "- Attention weights show what the model \"focuses on\"\n",
    "- Useful for debugging and understanding\n",
    "\n",
    "### 3. **Efficient Parallelization**\n",
    "- All computations can be done in parallel\n",
    "- Excellent for modern GPU architectures\n",
    "\n",
    "### 4. **Universal Approximation**\n",
    "- With enough heads and layers, can model complex functions\n",
    "- Foundation for modern language models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def standard_attention(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, \n",
    "                      causal: bool = False) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Standard attention implementation - stores full attention matrix.\n",
    "    \n",
    "    Memory complexity: O(L²) where L is sequence length\n",
    "    \"\"\"\n",
    "    B, H, L, D = Q.shape\n",
    "    scale = 1.0 / (D ** 0.5)\n",
    "    \n",
    "    # Step 1: Compute attention scores [B, H, L, L]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "    \n",
    "    # Step 2: Apply causal mask if needed\n",
    "    if causal:\n",
    "        mask = torch.triu(torch.ones(L, L, device=Q.device, dtype=torch.bool), diagonal=1)\n",
    "        scores.masked_fill_(mask, float('-inf'))\n",
    "    \n",
    "    # Step 3: Softmax - still O(L²) memory\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Apply to values\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Test parameters\n",
    "batch_size = 1\n",
    "n_heads = 1\n",
    "seq_len = 4\n",
    "head_dim = 4\n",
    "\n",
    "print(f\"Test configuration:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Number of heads: {n_heads}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Head dimension: {head_dim}\")\n",
    "print()\n",
    "\n",
    "# Create test data\n",
    "torch.manual_seed(42)  # For reproducible results\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, head_dim)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, head_dim)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, head_dim)\n",
    "\n",
    "print(\"Input tensor shapes:\")\n",
    "print(f\"  Q: {Q.shape}\")\n",
    "print(f\"  K: {K.shape}\")\n",
    "print(f\"  V: {V.shape}\")\n",
    "print()\n",
    "\n",
    "# Test 1: Regular attention (no causal mask)\n",
    "print(\"Test 1: Regular Attention\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "output, attn_weights = standard_attention(Q, K, V, causal=False)\n",
    "\n",
    "print(f\"SHAPE: {attn_weights.shape}\")\n",
    "pprint.pprint(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13eb840",
   "metadata": {},
   "source": [
    "# Vanilla Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b50f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MODEL_PARAMS\n",
    "from basic_attention.vanilla_transformer import VanillaTransformer\n",
    "from tokenizer import SimpleTokenizer\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = MODEL_PARAMS.get('VOCAB', 1000)\n",
    "D_MODEL = MODEL_PARAMS.get('D_MODEL', 256)\n",
    "N_HEADS = MODEL_PARAMS.get('N_HEADS', 8)\n",
    "N_LAYERS = 1\n",
    "SEQ_LEN = MODEL_PARAMS.get('SEQ_LEN', 64)\n",
    "\n",
    "transformer = VanillaTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=D_MODEL,\n",
    "        n_heads=N_HEADS,\n",
    "        n_layers=N_LAYERS,\n",
    "        max_seq_len=SEQ_LEN,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "print(f\"Model parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Test data\n",
    "tokenizer = SimpleTokenizer(VOCAB_SIZE)\n",
    "text = \"Hello world, welcome to this lecture series! -> Alexy\"\n",
    "print(f\"\\nInput text: '{text}'\")\n",
    "\n",
    "# Step 1: Tokenize\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Step 2: Pad sequence to fixed length\n",
    "if len(token_ids) < SEQ_LEN:\n",
    "    token_ids.extend([tokenizer.pad_token] * (SEQ_LEN - len(token_ids)))\n",
    "else:\n",
    "    token_ids = token_ids[:SEQ_LEN]\n",
    "\n",
    "# Step 3: Convert to tensor and add batch dimension\n",
    "tokens = torch.tensor([token_ids], device=device)  # [1, SEQ_LEN]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Regular attention\n",
    "    context = transformer(tokens, causal=False)\n",
    "    print(f\"Output shape: {context.shape}\")\n",
    "    pprint.pprint(f\"Context Vector {context}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92e934",
   "metadata": {},
   "source": [
    "# Flash Attention Implementation\n",
    "## Blog Reference\n",
    "- https://christianjmills.com/posts/cuda-mode-notes/lecture-012/\n",
    "- https://alexdremov.me/understanding-flash-attention-writing-the-algorithm-from-scratch-in-triton/\n",
    "- https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad\n",
    "- https://github.com/Dao-AILab/flash-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0404d",
   "metadata": {},
   "source": [
    "# Flash Attention: How does it work?\n",
    "## The Problem\n",
    "\n",
    "**Standard attention** computes and stores the full attention matrix in memory:\n",
    "\n",
    "```\n",
    "Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "```\n",
    "\n",
    "- **Memory bottleneck**: O(L²) for attention matrix storage\n",
    "- **Example**: L=4096 → ~1GB just for attention weights\n",
    "- **Result**: Can't scale to long sequences\n",
    "\n",
    "---\n",
    "\n",
    "## The Flash Attention Solution\n",
    "\n",
    "**Key insight**: Don't store the full attention matrix. Compute it in blocks.\n",
    "\n",
    "### Core Innovation: Block-wise Computation\n",
    "\n",
    "1. **Tile the computation**: Process attention in small blocks\n",
    "2. **Online softmax**: Update statistics incrementally  \n",
    "3. **Never materialize**: Full attention matrix never stored\n",
    "4. **Memory reduction**: O(L²) → O(L)\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "```python\n",
    "# Instead of:\n",
    "scores = Q @ K^T              # O(L²) memory\n",
    "weights = softmax(scores)     # O(L²) memory  \n",
    "output = weights @ V          # Standard\n",
    "\n",
    "# Flash Attention does:\n",
    "for query_block in Q:\n",
    "    for key_block, value_block in zip(K, V):\n",
    "        # Compute small block-wise attention\n",
    "        # Update running statistics (online softmax)\n",
    "        # Accumulate partial outputs\n",
    "        # Never store full matrix!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Equivalence\n",
    "\n",
    "**Crucial property**: Flash Attention produces **identical results** to standard attention.\n",
    "\n",
    "- **Same softmax formula**\n",
    "- **Same attention weights** (computed differently)\n",
    "- **Same final output** (mathematically proven)\n",
    "- **Zero approximation** - exact computation\n",
    "\n",
    "---\n",
    "\n",
    "## Memory Comparison\n",
    "\n",
    "| Sequence Length | Standard Attention | Flash Attention | Reduction |\n",
    "|-----------------|-------------------|-----------------|-----------|\n",
    "| 1,024          | 64 MB             | 4 MB            | 16x       |\n",
    "| 4,096          | 1 GB              | 16 MB           | 64x       |\n",
    "| 8,192          | 4 GB              | 32 MB           | 128x      |\n",
    "\n",
    "---\n",
    "\n",
    "## The Online Softmax Trick\n",
    "\n",
    "**Challenge**: How to compute softmax without storing all values?\n",
    "\n",
    "**Solution**: Update running statistics incrementally:\n",
    "\n",
    "```python\n",
    "# For each new block of scores:\n",
    "m_new = max(m_old, scores.max())           # Update max\n",
    "exp_scores = exp(scores - m_new)           # Rescale\n",
    "l_new = exp(m_old - m_new) * l_old + exp_scores.sum()  # Update sum\n",
    "\n",
    "# Final: softmax = exp(scores - m_new) / l_new\n",
    "```\n",
    "\n",
    "This allows computing exact softmax probabilities without storing the full matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "### Two-Pass Algorithm:\n",
    "\n",
    "**Pass 1**: Compute row-wise statistics\n",
    "- Find maximum score per query\n",
    "- Compute row-wise normalizing constants\n",
    "\n",
    "**Pass 2**: Compute actual output\n",
    "- Recompute scores block-by-block\n",
    "- Apply exact softmax using Pass 1 statistics\n",
    "- Accumulate weighted values\n",
    "\n",
    "### Block Size Tuning:\n",
    "- **Larger blocks**: Better compute efficiency\n",
    "- **Smaller blocks**: Lower memory usage\n",
    "- **Typical choice**: 64x64 or 128x128 blocks\n",
    "\n",
    "---\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "### ✅ **Memory Efficiency**\n",
    "- **O(L) instead of O(L²)** memory usage\n",
    "- Enables much longer sequences\n",
    "- Better GPU memory utilization\n",
    "\n",
    "### ✅ **Speed Improvements**\n",
    "- **Memory-bound → compute-bound**\n",
    "- Better cache locality\n",
    "- Often faster despite more operations\n",
    "\n",
    "### ✅ **Mathematical Exactness**\n",
    "- **Zero approximation error**\n",
    "- Drop-in replacement for standard attention\n",
    "- Same gradients, same training dynamics\n",
    "\n",
    "### ✅ **Scalability**\n",
    "- **Enables modern LLMs** (GPT-4, LLaMA, etc.)\n",
    "- Longer context windows\n",
    "- Larger batch sizes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dcfd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizer import SimpleTokenizer\n",
    "from config import MODEL_PARAMS\n",
    "from flash_attention.flash_attention_transformer import FlashAttentionTransformer\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Model parameters\n",
    "VOCAB_SIZE = MODEL_PARAMS.get('VOCAB', 1000)\n",
    "D_MODEL = MODEL_PARAMS.get('D_MODEL', 256)\n",
    "N_HEADS = MODEL_PARAMS.get('N_HEADS', 8)\n",
    "N_LAYERS = 1  # Multiple layers\n",
    "SEQ_LEN = MODEL_PARAMS.get('SEQ_LEN', 64)\n",
    "\n",
    "\n",
    "# Create complete Flash Attention transformer\n",
    "transformer = FlashAttentionTransformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    n_layers=N_LAYERS,\n",
    "    dropout=0.1,\n",
    "    debug=True  # Set to True to see attention visualizations\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "print(f\"Model device: {next(transformer.parameters()).device}\")\n",
    "\n",
    "# Test data\n",
    "tokenizer = SimpleTokenizer(VOCAB_SIZE)\n",
    "text = \"Hello world, welcome to this lecture series from Nice Intelligence\"\n",
    "print(f\"\\nInput text: '{text}'\")\n",
    "\n",
    "# Tokenize and prepare\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Pad to sequence length\n",
    "if len(token_ids) < SEQ_LEN:\n",
    "    token_ids.extend([tokenizer.pad_token] * (SEQ_LEN - len(token_ids)))\n",
    "else:\n",
    "    token_ids = token_ids[:SEQ_LEN]\n",
    "\n",
    "tokens = torch.tensor([token_ids], device=device)  # [1, SEQ_LEN]\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    context = transformer(tokens, causal=False)\n",
    "    print(f\"Context shape: {context.shape}\")\n",
    "    print(f\"Context dtype: {context.dtype}\")\n",
    "    print(f\"Sample context: {context[0, 0, :6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab736035",
   "metadata": {},
   "source": [
    "# Benchmarking Flash Attention\n",
    "- https://colab.research.google.com/drive/1NgoOfJbQRGOsJh1aBOghj5rTn6m2XhnS#scrollTo=oX42RVTkCiz8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
